{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.226510827217541,
  "eval_steps": 500,
  "global_step": 2500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.000906043308870164,
      "grad_norm": 5.058318138122559,
      "learning_rate": 4.9833333333333336e-05,
      "loss": 4.2996,
      "step": 10
    },
    {
      "epoch": 0.001812086617740328,
      "grad_norm": 4.612018585205078,
      "learning_rate": 4.966666666666667e-05,
      "loss": 4.5566,
      "step": 20
    },
    {
      "epoch": 0.002718129926610492,
      "grad_norm": 5.93319034576416,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 4.4918,
      "step": 30
    },
    {
      "epoch": 0.003624173235480656,
      "grad_norm": 5.665061950683594,
      "learning_rate": 4.933333333333334e-05,
      "loss": 4.1873,
      "step": 40
    },
    {
      "epoch": 0.00453021654435082,
      "grad_norm": 8.631671905517578,
      "learning_rate": 4.9166666666666665e-05,
      "loss": 3.9963,
      "step": 50
    },
    {
      "epoch": 0.005436259853220984,
      "grad_norm": 8.569334030151367,
      "learning_rate": 4.9e-05,
      "loss": 3.5424,
      "step": 60
    },
    {
      "epoch": 0.006342303162091148,
      "grad_norm": 10.963932037353516,
      "learning_rate": 4.883333333333334e-05,
      "loss": 3.8777,
      "step": 70
    },
    {
      "epoch": 0.007248346470961312,
      "grad_norm": 11.362627983093262,
      "learning_rate": 4.866666666666667e-05,
      "loss": 3.5795,
      "step": 80
    },
    {
      "epoch": 0.008154389779831475,
      "grad_norm": 7.5196003913879395,
      "learning_rate": 4.85e-05,
      "loss": 3.3719,
      "step": 90
    },
    {
      "epoch": 0.00906043308870164,
      "grad_norm": 8.712883949279785,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 3.4102,
      "step": 100
    },
    {
      "epoch": 0.009966476397571804,
      "grad_norm": 3.482051372528076,
      "learning_rate": 4.8166666666666674e-05,
      "loss": 3.0368,
      "step": 110
    },
    {
      "epoch": 0.010872519706441968,
      "grad_norm": 9.76336669921875,
      "learning_rate": 4.8e-05,
      "loss": 2.9068,
      "step": 120
    },
    {
      "epoch": 0.011778563015312132,
      "grad_norm": 7.592294216156006,
      "learning_rate": 4.7833333333333335e-05,
      "loss": 3.351,
      "step": 130
    },
    {
      "epoch": 0.012684606324182297,
      "grad_norm": 7.952542781829834,
      "learning_rate": 4.766666666666667e-05,
      "loss": 3.0678,
      "step": 140
    },
    {
      "epoch": 0.01359064963305246,
      "grad_norm": 8.474177360534668,
      "learning_rate": 4.75e-05,
      "loss": 2.9259,
      "step": 150
    },
    {
      "epoch": 0.014496692941922623,
      "grad_norm": 11.980826377868652,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 2.7715,
      "step": 160
    },
    {
      "epoch": 0.015402736250792788,
      "grad_norm": 7.603084564208984,
      "learning_rate": 4.716666666666667e-05,
      "loss": 2.8369,
      "step": 170
    },
    {
      "epoch": 0.01630877955966295,
      "grad_norm": 10.271730422973633,
      "learning_rate": 4.7e-05,
      "loss": 2.3388,
      "step": 180
    },
    {
      "epoch": 0.017214822868533115,
      "grad_norm": 8.600616455078125,
      "learning_rate": 4.683333333333334e-05,
      "loss": 2.564,
      "step": 190
    },
    {
      "epoch": 0.01812086617740328,
      "grad_norm": 12.394204139709473,
      "learning_rate": 4.666666666666667e-05,
      "loss": 2.5828,
      "step": 200
    },
    {
      "epoch": 0.019026909486273443,
      "grad_norm": 9.47671127319336,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 2.3654,
      "step": 210
    },
    {
      "epoch": 0.019932952795143608,
      "grad_norm": 10.760744094848633,
      "learning_rate": 4.633333333333333e-05,
      "loss": 2.874,
      "step": 220
    },
    {
      "epoch": 0.020838996104013772,
      "grad_norm": 9.09969711303711,
      "learning_rate": 4.6166666666666666e-05,
      "loss": 2.5251,
      "step": 230
    },
    {
      "epoch": 0.021745039412883936,
      "grad_norm": 15.933286666870117,
      "learning_rate": 4.600000000000001e-05,
      "loss": 2.6729,
      "step": 240
    },
    {
      "epoch": 0.0226510827217541,
      "grad_norm": 9.17481517791748,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 2.8799,
      "step": 250
    },
    {
      "epoch": 0.023557126030624265,
      "grad_norm": 10.115382194519043,
      "learning_rate": 4.566666666666667e-05,
      "loss": 2.9363,
      "step": 260
    },
    {
      "epoch": 0.02446316933949443,
      "grad_norm": 9.714411735534668,
      "learning_rate": 4.55e-05,
      "loss": 2.6101,
      "step": 270
    },
    {
      "epoch": 0.025369212648364593,
      "grad_norm": 11.71825885772705,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 2.1794,
      "step": 280
    },
    {
      "epoch": 0.026275255957234754,
      "grad_norm": 10.637216567993164,
      "learning_rate": 4.516666666666667e-05,
      "loss": 2.447,
      "step": 290
    },
    {
      "epoch": 0.02718129926610492,
      "grad_norm": 10.136570930480957,
      "learning_rate": 4.5e-05,
      "loss": 2.6656,
      "step": 300
    },
    {
      "epoch": 0.028087342574975083,
      "grad_norm": 9.51448917388916,
      "learning_rate": 4.483333333333333e-05,
      "loss": 2.2594,
      "step": 310
    },
    {
      "epoch": 0.028993385883845247,
      "grad_norm": 10.981176376342773,
      "learning_rate": 4.466666666666667e-05,
      "loss": 2.6132,
      "step": 320
    },
    {
      "epoch": 0.02989942919271541,
      "grad_norm": 11.329806327819824,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 2.4153,
      "step": 330
    },
    {
      "epoch": 0.030805472501585576,
      "grad_norm": 10.317923545837402,
      "learning_rate": 4.433333333333334e-05,
      "loss": 2.1514,
      "step": 340
    },
    {
      "epoch": 0.03171151581045574,
      "grad_norm": 15.368780136108398,
      "learning_rate": 4.4166666666666665e-05,
      "loss": 2.6028,
      "step": 350
    },
    {
      "epoch": 0.0326175591193259,
      "grad_norm": 9.428500175476074,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 2.4376,
      "step": 360
    },
    {
      "epoch": 0.03352360242819607,
      "grad_norm": 7.910420894622803,
      "learning_rate": 4.383333333333334e-05,
      "loss": 2.4272,
      "step": 370
    },
    {
      "epoch": 0.03442964573706623,
      "grad_norm": 10.343652725219727,
      "learning_rate": 4.3666666666666666e-05,
      "loss": 2.4721,
      "step": 380
    },
    {
      "epoch": 0.0353356890459364,
      "grad_norm": 9.59859848022461,
      "learning_rate": 4.35e-05,
      "loss": 2.4086,
      "step": 390
    },
    {
      "epoch": 0.03624173235480656,
      "grad_norm": 10.581686973571777,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 2.2918,
      "step": 400
    },
    {
      "epoch": 0.037147775663676726,
      "grad_norm": 7.394169807434082,
      "learning_rate": 4.316666666666667e-05,
      "loss": 2.1225,
      "step": 410
    },
    {
      "epoch": 0.038053818972546886,
      "grad_norm": 7.77184534072876,
      "learning_rate": 4.3e-05,
      "loss": 2.5354,
      "step": 420
    },
    {
      "epoch": 0.038959862281417054,
      "grad_norm": 10.436568260192871,
      "learning_rate": 4.2833333333333335e-05,
      "loss": 2.376,
      "step": 430
    },
    {
      "epoch": 0.039865905590287215,
      "grad_norm": 9.167475700378418,
      "learning_rate": 4.266666666666667e-05,
      "loss": 2.3035,
      "step": 440
    },
    {
      "epoch": 0.04077194889915738,
      "grad_norm": 8.736044883728027,
      "learning_rate": 4.25e-05,
      "loss": 2.3692,
      "step": 450
    },
    {
      "epoch": 0.041677992208027544,
      "grad_norm": 8.844491958618164,
      "learning_rate": 4.233333333333334e-05,
      "loss": 2.2739,
      "step": 460
    },
    {
      "epoch": 0.042584035516897704,
      "grad_norm": 12.017705917358398,
      "learning_rate": 4.216666666666667e-05,
      "loss": 2.3418,
      "step": 470
    },
    {
      "epoch": 0.04349007882576787,
      "grad_norm": 7.705962657928467,
      "learning_rate": 4.2e-05,
      "loss": 2.4991,
      "step": 480
    },
    {
      "epoch": 0.04439612213463803,
      "grad_norm": 8.480086326599121,
      "learning_rate": 4.183333333333334e-05,
      "loss": 2.5204,
      "step": 490
    },
    {
      "epoch": 0.0453021654435082,
      "grad_norm": 11.11162281036377,
      "learning_rate": 4.166666666666667e-05,
      "loss": 1.9123,
      "step": 500
    },
    {
      "epoch": 0.0453021654435082,
      "eval_bleu-4": 0.18496605620376852,
      "eval_rouge-1": 46.108366000000004,
      "eval_rouge-2": 23.210758000000002,
      "eval_rouge-l": 45.00054599999999,
      "eval_runtime": 9.309,
      "eval_samples_per_second": 5.371,
      "eval_steps_per_second": 0.43,
      "step": 500
    },
    {
      "epoch": 0.04620820875237836,
      "grad_norm": 12.680835723876953,
      "learning_rate": 4.15e-05,
      "loss": 2.266,
      "step": 510
    },
    {
      "epoch": 0.04711425206124853,
      "grad_norm": 6.331789493560791,
      "learning_rate": 4.133333333333333e-05,
      "loss": 2.493,
      "step": 520
    },
    {
      "epoch": 0.04802029537011869,
      "grad_norm": 11.903470993041992,
      "learning_rate": 4.116666666666667e-05,
      "loss": 2.0826,
      "step": 530
    },
    {
      "epoch": 0.04892633867898886,
      "grad_norm": 10.76155948638916,
      "learning_rate": 4.1e-05,
      "loss": 2.1889,
      "step": 540
    },
    {
      "epoch": 0.04983238198785902,
      "grad_norm": 12.963102340698242,
      "learning_rate": 4.0833333333333334e-05,
      "loss": 2.5938,
      "step": 550
    },
    {
      "epoch": 0.05073842529672919,
      "grad_norm": 10.819414138793945,
      "learning_rate": 4.066666666666667e-05,
      "loss": 2.1481,
      "step": 560
    },
    {
      "epoch": 0.05164446860559935,
      "grad_norm": 13.27579402923584,
      "learning_rate": 4.05e-05,
      "loss": 2.3975,
      "step": 570
    },
    {
      "epoch": 0.05255051191446951,
      "grad_norm": 7.28256368637085,
      "learning_rate": 4.0333333333333336e-05,
      "loss": 2.4572,
      "step": 580
    },
    {
      "epoch": 0.053456555223339676,
      "grad_norm": 8.7088623046875,
      "learning_rate": 4.016666666666667e-05,
      "loss": 2.4609,
      "step": 590
    },
    {
      "epoch": 0.05436259853220984,
      "grad_norm": 13.281689643859863,
      "learning_rate": 4e-05,
      "loss": 2.3788,
      "step": 600
    },
    {
      "epoch": 0.055268641841080005,
      "grad_norm": 8.40445613861084,
      "learning_rate": 3.983333333333333e-05,
      "loss": 2.2704,
      "step": 610
    },
    {
      "epoch": 0.056174685149950165,
      "grad_norm": 9.03034782409668,
      "learning_rate": 3.966666666666667e-05,
      "loss": 2.2546,
      "step": 620
    },
    {
      "epoch": 0.05708072845882033,
      "grad_norm": 10.26009750366211,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 2.2623,
      "step": 630
    },
    {
      "epoch": 0.057986771767690494,
      "grad_norm": 13.158516883850098,
      "learning_rate": 3.933333333333333e-05,
      "loss": 2.1886,
      "step": 640
    },
    {
      "epoch": 0.05889281507656066,
      "grad_norm": 15.890116691589355,
      "learning_rate": 3.9166666666666665e-05,
      "loss": 2.0648,
      "step": 650
    },
    {
      "epoch": 0.05979885838543082,
      "grad_norm": 10.659934997558594,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 2.2956,
      "step": 660
    },
    {
      "epoch": 0.06070490169430099,
      "grad_norm": 7.520054817199707,
      "learning_rate": 3.883333333333333e-05,
      "loss": 2.6994,
      "step": 670
    },
    {
      "epoch": 0.06161094500317115,
      "grad_norm": 11.663171768188477,
      "learning_rate": 3.866666666666667e-05,
      "loss": 2.3229,
      "step": 680
    },
    {
      "epoch": 0.06251698831204132,
      "grad_norm": 12.052133560180664,
      "learning_rate": 3.85e-05,
      "loss": 2.6508,
      "step": 690
    },
    {
      "epoch": 0.06342303162091148,
      "grad_norm": 14.419153213500977,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 2.4578,
      "step": 700
    },
    {
      "epoch": 0.06432907492978164,
      "grad_norm": 9.739251136779785,
      "learning_rate": 3.816666666666667e-05,
      "loss": 2.4231,
      "step": 710
    },
    {
      "epoch": 0.0652351182386518,
      "grad_norm": 9.84488296508789,
      "learning_rate": 3.8e-05,
      "loss": 2.4117,
      "step": 720
    },
    {
      "epoch": 0.06614116154752198,
      "grad_norm": 10.318965911865234,
      "learning_rate": 3.7833333333333336e-05,
      "loss": 2.7389,
      "step": 730
    },
    {
      "epoch": 0.06704720485639214,
      "grad_norm": 13.418259620666504,
      "learning_rate": 3.766666666666667e-05,
      "loss": 2.3415,
      "step": 740
    },
    {
      "epoch": 0.0679532481652623,
      "grad_norm": 14.711467742919922,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 1.9604,
      "step": 750
    },
    {
      "epoch": 0.06885929147413246,
      "grad_norm": 14.460500717163086,
      "learning_rate": 3.733333333333334e-05,
      "loss": 2.7011,
      "step": 760
    },
    {
      "epoch": 0.06976533478300263,
      "grad_norm": 9.960517883300781,
      "learning_rate": 3.7166666666666664e-05,
      "loss": 1.9842,
      "step": 770
    },
    {
      "epoch": 0.0706713780918728,
      "grad_norm": 8.573473930358887,
      "learning_rate": 3.7e-05,
      "loss": 2.322,
      "step": 780
    },
    {
      "epoch": 0.07157742140074295,
      "grad_norm": 12.293778419494629,
      "learning_rate": 3.683333333333334e-05,
      "loss": 2.0804,
      "step": 790
    },
    {
      "epoch": 0.07248346470961312,
      "grad_norm": 10.140371322631836,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 2.5539,
      "step": 800
    },
    {
      "epoch": 0.07338950801848329,
      "grad_norm": 11.765308380126953,
      "learning_rate": 3.65e-05,
      "loss": 2.1507,
      "step": 810
    },
    {
      "epoch": 0.07429555132735345,
      "grad_norm": 9.751572608947754,
      "learning_rate": 3.633333333333333e-05,
      "loss": 2.364,
      "step": 820
    },
    {
      "epoch": 0.07520159463622361,
      "grad_norm": 16.334360122680664,
      "learning_rate": 3.6166666666666674e-05,
      "loss": 2.3252,
      "step": 830
    },
    {
      "epoch": 0.07610763794509377,
      "grad_norm": 10.269999504089355,
      "learning_rate": 3.6e-05,
      "loss": 2.3,
      "step": 840
    },
    {
      "epoch": 0.07701368125396393,
      "grad_norm": 12.02754020690918,
      "learning_rate": 3.5833333333333335e-05,
      "loss": 2.0976,
      "step": 850
    },
    {
      "epoch": 0.07791972456283411,
      "grad_norm": 9.18252182006836,
      "learning_rate": 3.566666666666667e-05,
      "loss": 1.9679,
      "step": 860
    },
    {
      "epoch": 0.07882576787170427,
      "grad_norm": 9.564445495605469,
      "learning_rate": 3.55e-05,
      "loss": 1.9206,
      "step": 870
    },
    {
      "epoch": 0.07973181118057443,
      "grad_norm": 10.675238609313965,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 2.18,
      "step": 880
    },
    {
      "epoch": 0.08063785448944459,
      "grad_norm": 11.756234169006348,
      "learning_rate": 3.516666666666667e-05,
      "loss": 2.4796,
      "step": 890
    },
    {
      "epoch": 0.08154389779831477,
      "grad_norm": 8.418935775756836,
      "learning_rate": 3.5e-05,
      "loss": 2.201,
      "step": 900
    },
    {
      "epoch": 0.08244994110718493,
      "grad_norm": 15.207225799560547,
      "learning_rate": 3.483333333333334e-05,
      "loss": 2.1888,
      "step": 910
    },
    {
      "epoch": 0.08335598441605509,
      "grad_norm": 11.317954063415527,
      "learning_rate": 3.466666666666667e-05,
      "loss": 2.6254,
      "step": 920
    },
    {
      "epoch": 0.08426202772492525,
      "grad_norm": 8.814838409423828,
      "learning_rate": 3.45e-05,
      "loss": 1.9874,
      "step": 930
    },
    {
      "epoch": 0.08516807103379541,
      "grad_norm": 12.62546443939209,
      "learning_rate": 3.433333333333333e-05,
      "loss": 2.4539,
      "step": 940
    },
    {
      "epoch": 0.08607411434266558,
      "grad_norm": 11.72036361694336,
      "learning_rate": 3.4166666666666666e-05,
      "loss": 2.3764,
      "step": 950
    },
    {
      "epoch": 0.08698015765153574,
      "grad_norm": 12.788076400756836,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 2.104,
      "step": 960
    },
    {
      "epoch": 0.0878862009604059,
      "grad_norm": 9.055018424987793,
      "learning_rate": 3.3833333333333334e-05,
      "loss": 1.9733,
      "step": 970
    },
    {
      "epoch": 0.08879224426927607,
      "grad_norm": 14.931652069091797,
      "learning_rate": 3.366666666666667e-05,
      "loss": 2.489,
      "step": 980
    },
    {
      "epoch": 0.08969828757814624,
      "grad_norm": 10.764139175415039,
      "learning_rate": 3.35e-05,
      "loss": 2.3498,
      "step": 990
    },
    {
      "epoch": 0.0906043308870164,
      "grad_norm": 13.736348152160645,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 2.5522,
      "step": 1000
    },
    {
      "epoch": 0.0906043308870164,
      "eval_bleu-4": 0.2047752929707931,
      "eval_rouge-1": 48.32246,
      "eval_rouge-2": 25.152704,
      "eval_rouge-l": 48.07611,
      "eval_runtime": 8.8506,
      "eval_samples_per_second": 5.649,
      "eval_steps_per_second": 0.452,
      "step": 1000
    },
    {
      "epoch": 0.09151037419588656,
      "grad_norm": 11.236151695251465,
      "learning_rate": 3.316666666666667e-05,
      "loss": 2.1536,
      "step": 1010
    },
    {
      "epoch": 0.09241641750475672,
      "grad_norm": 10.103128433227539,
      "learning_rate": 3.3e-05,
      "loss": 2.1893,
      "step": 1020
    },
    {
      "epoch": 0.0933224608136269,
      "grad_norm": 8.02674388885498,
      "learning_rate": 3.283333333333333e-05,
      "loss": 2.1695,
      "step": 1030
    },
    {
      "epoch": 0.09422850412249706,
      "grad_norm": 13.887248992919922,
      "learning_rate": 3.266666666666667e-05,
      "loss": 2.1174,
      "step": 1040
    },
    {
      "epoch": 0.09513454743136722,
      "grad_norm": 11.391569137573242,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 2.1511,
      "step": 1050
    },
    {
      "epoch": 0.09604059074023738,
      "grad_norm": 10.5208740234375,
      "learning_rate": 3.233333333333333e-05,
      "loss": 2.3046,
      "step": 1060
    },
    {
      "epoch": 0.09694663404910754,
      "grad_norm": 14.101656913757324,
      "learning_rate": 3.2166666666666665e-05,
      "loss": 2.4059,
      "step": 1070
    },
    {
      "epoch": 0.09785267735797772,
      "grad_norm": 8.555663108825684,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 2.0145,
      "step": 1080
    },
    {
      "epoch": 0.09875872066684788,
      "grad_norm": 11.93560791015625,
      "learning_rate": 3.183333333333334e-05,
      "loss": 2.0328,
      "step": 1090
    },
    {
      "epoch": 0.09966476397571804,
      "grad_norm": 8.905778884887695,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 2.2791,
      "step": 1100
    },
    {
      "epoch": 0.1005708072845882,
      "grad_norm": 12.196864128112793,
      "learning_rate": 3.15e-05,
      "loss": 2.1197,
      "step": 1110
    },
    {
      "epoch": 0.10147685059345837,
      "grad_norm": 10.203831672668457,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 2.1939,
      "step": 1120
    },
    {
      "epoch": 0.10238289390232853,
      "grad_norm": 11.424958229064941,
      "learning_rate": 3.116666666666667e-05,
      "loss": 2.3383,
      "step": 1130
    },
    {
      "epoch": 0.1032889372111987,
      "grad_norm": 9.555253028869629,
      "learning_rate": 3.1e-05,
      "loss": 2.374,
      "step": 1140
    },
    {
      "epoch": 0.10419498052006886,
      "grad_norm": 12.010000228881836,
      "learning_rate": 3.0833333333333335e-05,
      "loss": 2.3706,
      "step": 1150
    },
    {
      "epoch": 0.10510102382893902,
      "grad_norm": 13.469535827636719,
      "learning_rate": 3.066666666666667e-05,
      "loss": 2.2827,
      "step": 1160
    },
    {
      "epoch": 0.10600706713780919,
      "grad_norm": 12.791735649108887,
      "learning_rate": 3.05e-05,
      "loss": 2.4362,
      "step": 1170
    },
    {
      "epoch": 0.10691311044667935,
      "grad_norm": 11.947844505310059,
      "learning_rate": 3.0333333333333337e-05,
      "loss": 1.8647,
      "step": 1180
    },
    {
      "epoch": 0.10781915375554951,
      "grad_norm": 6.165929794311523,
      "learning_rate": 3.016666666666667e-05,
      "loss": 2.4146,
      "step": 1190
    },
    {
      "epoch": 0.10872519706441967,
      "grad_norm": 10.078563690185547,
      "learning_rate": 3e-05,
      "loss": 2.142,
      "step": 1200
    },
    {
      "epoch": 0.10963124037328985,
      "grad_norm": 7.388530731201172,
      "learning_rate": 2.9833333333333335e-05,
      "loss": 2.4946,
      "step": 1210
    },
    {
      "epoch": 0.11053728368216001,
      "grad_norm": 8.325481414794922,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 2.3647,
      "step": 1220
    },
    {
      "epoch": 0.11144332699103017,
      "grad_norm": 6.906781196594238,
      "learning_rate": 2.95e-05,
      "loss": 1.9643,
      "step": 1230
    },
    {
      "epoch": 0.11234937029990033,
      "grad_norm": 14.038446426391602,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 2.5034,
      "step": 1240
    },
    {
      "epoch": 0.1132554136087705,
      "grad_norm": 11.834324836730957,
      "learning_rate": 2.916666666666667e-05,
      "loss": 2.1413,
      "step": 1250
    },
    {
      "epoch": 0.11416145691764067,
      "grad_norm": 8.870011329650879,
      "learning_rate": 2.9e-05,
      "loss": 2.3094,
      "step": 1260
    },
    {
      "epoch": 0.11506750022651083,
      "grad_norm": 9.783926963806152,
      "learning_rate": 2.8833333333333334e-05,
      "loss": 2.1011,
      "step": 1270
    },
    {
      "epoch": 0.11597354353538099,
      "grad_norm": 11.091421127319336,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 2.2328,
      "step": 1280
    },
    {
      "epoch": 0.11687958684425115,
      "grad_norm": 10.885375022888184,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 2.2294,
      "step": 1290
    },
    {
      "epoch": 0.11778563015312132,
      "grad_norm": 7.659281253814697,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 2.444,
      "step": 1300
    },
    {
      "epoch": 0.11869167346199148,
      "grad_norm": 11.791854858398438,
      "learning_rate": 2.816666666666667e-05,
      "loss": 2.0896,
      "step": 1310
    },
    {
      "epoch": 0.11959771677086165,
      "grad_norm": 14.45641040802002,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 2.102,
      "step": 1320
    },
    {
      "epoch": 0.1205037600797318,
      "grad_norm": 15.640124320983887,
      "learning_rate": 2.7833333333333333e-05,
      "loss": 2.2279,
      "step": 1330
    },
    {
      "epoch": 0.12140980338860198,
      "grad_norm": 7.5363383293151855,
      "learning_rate": 2.7666666666666667e-05,
      "loss": 2.1472,
      "step": 1340
    },
    {
      "epoch": 0.12231584669747214,
      "grad_norm": 12.162185668945312,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 2.3497,
      "step": 1350
    },
    {
      "epoch": 0.1232218900063423,
      "grad_norm": 9.103856086730957,
      "learning_rate": 2.733333333333333e-05,
      "loss": 2.2976,
      "step": 1360
    },
    {
      "epoch": 0.12412793331521246,
      "grad_norm": 14.426444053649902,
      "learning_rate": 2.716666666666667e-05,
      "loss": 2.232,
      "step": 1370
    },
    {
      "epoch": 0.12503397662408264,
      "grad_norm": 12.26777172088623,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 2.3755,
      "step": 1380
    },
    {
      "epoch": 0.12594001993295278,
      "grad_norm": 9.740706443786621,
      "learning_rate": 2.6833333333333333e-05,
      "loss": 1.8173,
      "step": 1390
    },
    {
      "epoch": 0.12684606324182296,
      "grad_norm": 11.19435977935791,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 2.5389,
      "step": 1400
    },
    {
      "epoch": 0.12775210655069313,
      "grad_norm": 8.680611610412598,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 2.2208,
      "step": 1410
    },
    {
      "epoch": 0.12865814985956328,
      "grad_norm": 10.937240600585938,
      "learning_rate": 2.633333333333333e-05,
      "loss": 2.4074,
      "step": 1420
    },
    {
      "epoch": 0.12956419316843346,
      "grad_norm": 13.59439468383789,
      "learning_rate": 2.6166666666666668e-05,
      "loss": 2.4676,
      "step": 1430
    },
    {
      "epoch": 0.1304702364773036,
      "grad_norm": 15.776057243347168,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.7788,
      "step": 1440
    },
    {
      "epoch": 0.13137627978617378,
      "grad_norm": 10.911452293395996,
      "learning_rate": 2.5833333333333336e-05,
      "loss": 1.9025,
      "step": 1450
    },
    {
      "epoch": 0.13228232309504395,
      "grad_norm": 10.88617992401123,
      "learning_rate": 2.5666666666666666e-05,
      "loss": 1.861,
      "step": 1460
    },
    {
      "epoch": 0.1331883664039141,
      "grad_norm": 10.265141487121582,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 2.0861,
      "step": 1470
    },
    {
      "epoch": 0.13409440971278427,
      "grad_norm": 11.468804359436035,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 2.264,
      "step": 1480
    },
    {
      "epoch": 0.13500045302165445,
      "grad_norm": 17.965913772583008,
      "learning_rate": 2.5166666666666667e-05,
      "loss": 2.0969,
      "step": 1490
    },
    {
      "epoch": 0.1359064963305246,
      "grad_norm": 10.823539733886719,
      "learning_rate": 2.5e-05,
      "loss": 2.2288,
      "step": 1500
    },
    {
      "epoch": 0.1359064963305246,
      "eval_bleu-4": 0.16904736494999065,
      "eval_rouge-1": 46.201416,
      "eval_rouge-2": 21.79395,
      "eval_rouge-l": 44.83633,
      "eval_runtime": 8.4275,
      "eval_samples_per_second": 5.933,
      "eval_steps_per_second": 0.475,
      "step": 1500
    },
    {
      "epoch": 0.13681253963939477,
      "grad_norm": 8.743132591247559,
      "learning_rate": 2.4833333333333335e-05,
      "loss": 2.0709,
      "step": 1510
    },
    {
      "epoch": 0.13771858294826492,
      "grad_norm": 15.680459976196289,
      "learning_rate": 2.466666666666667e-05,
      "loss": 2.4314,
      "step": 1520
    },
    {
      "epoch": 0.1386246262571351,
      "grad_norm": 9.720479011535645,
      "learning_rate": 2.45e-05,
      "loss": 2.0658,
      "step": 1530
    },
    {
      "epoch": 0.13953066956600527,
      "grad_norm": 6.091657638549805,
      "learning_rate": 2.4333333333333336e-05,
      "loss": 2.2089,
      "step": 1540
    },
    {
      "epoch": 0.1404367128748754,
      "grad_norm": 13.730778694152832,
      "learning_rate": 2.4166666666666667e-05,
      "loss": 2.661,
      "step": 1550
    },
    {
      "epoch": 0.1413427561837456,
      "grad_norm": 12.84874439239502,
      "learning_rate": 2.4e-05,
      "loss": 2.2812,
      "step": 1560
    },
    {
      "epoch": 0.14224879949261574,
      "grad_norm": 15.650192260742188,
      "learning_rate": 2.3833333333333334e-05,
      "loss": 2.3636,
      "step": 1570
    },
    {
      "epoch": 0.1431548428014859,
      "grad_norm": 16.297931671142578,
      "learning_rate": 2.3666666666666668e-05,
      "loss": 2.2146,
      "step": 1580
    },
    {
      "epoch": 0.14406088611035608,
      "grad_norm": 10.288654327392578,
      "learning_rate": 2.35e-05,
      "loss": 2.2691,
      "step": 1590
    },
    {
      "epoch": 0.14496692941922623,
      "grad_norm": 13.54492473602295,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 2.3304,
      "step": 1600
    },
    {
      "epoch": 0.1458729727280964,
      "grad_norm": 9.618468284606934,
      "learning_rate": 2.3166666666666666e-05,
      "loss": 2.3691,
      "step": 1610
    },
    {
      "epoch": 0.14677901603696658,
      "grad_norm": 11.981842041015625,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 2.2042,
      "step": 1620
    },
    {
      "epoch": 0.14768505934583673,
      "grad_norm": 6.109539031982422,
      "learning_rate": 2.2833333333333334e-05,
      "loss": 2.2464,
      "step": 1630
    },
    {
      "epoch": 0.1485911026547069,
      "grad_norm": 12.777978897094727,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 2.2033,
      "step": 1640
    },
    {
      "epoch": 0.14949714596357705,
      "grad_norm": 13.513893127441406,
      "learning_rate": 2.25e-05,
      "loss": 2.2427,
      "step": 1650
    },
    {
      "epoch": 0.15040318927244722,
      "grad_norm": 11.314863204956055,
      "learning_rate": 2.2333333333333335e-05,
      "loss": 2.3187,
      "step": 1660
    },
    {
      "epoch": 0.1513092325813174,
      "grad_norm": 7.966360569000244,
      "learning_rate": 2.216666666666667e-05,
      "loss": 2.5024,
      "step": 1670
    },
    {
      "epoch": 0.15221527589018755,
      "grad_norm": 8.428668975830078,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 2.3063,
      "step": 1680
    },
    {
      "epoch": 0.15312131919905772,
      "grad_norm": 12.631418228149414,
      "learning_rate": 2.1833333333333333e-05,
      "loss": 2.1404,
      "step": 1690
    },
    {
      "epoch": 0.15402736250792787,
      "grad_norm": 12.431632041931152,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 2.2148,
      "step": 1700
    },
    {
      "epoch": 0.15493340581679804,
      "grad_norm": 10.17529582977295,
      "learning_rate": 2.15e-05,
      "loss": 2.1375,
      "step": 1710
    },
    {
      "epoch": 0.15583944912566822,
      "grad_norm": 8.244682312011719,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 2.2206,
      "step": 1720
    },
    {
      "epoch": 0.15674549243453836,
      "grad_norm": 12.793545722961426,
      "learning_rate": 2.116666666666667e-05,
      "loss": 2.2778,
      "step": 1730
    },
    {
      "epoch": 0.15765153574340854,
      "grad_norm": 9.067354202270508,
      "learning_rate": 2.1e-05,
      "loss": 2.0275,
      "step": 1740
    },
    {
      "epoch": 0.15855757905227869,
      "grad_norm": 19.860475540161133,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 2.5037,
      "step": 1750
    },
    {
      "epoch": 0.15946362236114886,
      "grad_norm": 12.205263137817383,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 2.1602,
      "step": 1760
    },
    {
      "epoch": 0.16036966567001903,
      "grad_norm": 13.814645767211914,
      "learning_rate": 2.05e-05,
      "loss": 1.5644,
      "step": 1770
    },
    {
      "epoch": 0.16127570897888918,
      "grad_norm": 9.594318389892578,
      "learning_rate": 2.0333333333333334e-05,
      "loss": 2.3939,
      "step": 1780
    },
    {
      "epoch": 0.16218175228775936,
      "grad_norm": 14.800263404846191,
      "learning_rate": 2.0166666666666668e-05,
      "loss": 2.0385,
      "step": 1790
    },
    {
      "epoch": 0.16308779559662953,
      "grad_norm": 12.732799530029297,
      "learning_rate": 2e-05,
      "loss": 2.1275,
      "step": 1800
    },
    {
      "epoch": 0.16399383890549968,
      "grad_norm": 10.753396034240723,
      "learning_rate": 1.9833333333333335e-05,
      "loss": 1.8268,
      "step": 1810
    },
    {
      "epoch": 0.16489988221436985,
      "grad_norm": 7.517484188079834,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 2.1055,
      "step": 1820
    },
    {
      "epoch": 0.16580592552324,
      "grad_norm": 21.51384162902832,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 2.2467,
      "step": 1830
    },
    {
      "epoch": 0.16671196883211017,
      "grad_norm": 8.46928596496582,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 2.0651,
      "step": 1840
    },
    {
      "epoch": 0.16761801214098035,
      "grad_norm": 8.335135459899902,
      "learning_rate": 1.9166666666666667e-05,
      "loss": 2.2994,
      "step": 1850
    },
    {
      "epoch": 0.1685240554498505,
      "grad_norm": 10.612390518188477,
      "learning_rate": 1.9e-05,
      "loss": 2.0073,
      "step": 1860
    },
    {
      "epoch": 0.16943009875872067,
      "grad_norm": 12.229158401489258,
      "learning_rate": 1.8833333333333335e-05,
      "loss": 2.359,
      "step": 1870
    },
    {
      "epoch": 0.17033614206759082,
      "grad_norm": 12.919897079467773,
      "learning_rate": 1.866666666666667e-05,
      "loss": 1.9632,
      "step": 1880
    },
    {
      "epoch": 0.171242185376461,
      "grad_norm": 8.316494941711426,
      "learning_rate": 1.85e-05,
      "loss": 2.2568,
      "step": 1890
    },
    {
      "epoch": 0.17214822868533117,
      "grad_norm": 9.448884010314941,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 2.1812,
      "step": 1900
    },
    {
      "epoch": 0.17305427199420131,
      "grad_norm": 11.456542015075684,
      "learning_rate": 1.8166666666666667e-05,
      "loss": 2.0154,
      "step": 1910
    },
    {
      "epoch": 0.1739603153030715,
      "grad_norm": 10.005935668945312,
      "learning_rate": 1.8e-05,
      "loss": 2.0125,
      "step": 1920
    },
    {
      "epoch": 0.17486635861194166,
      "grad_norm": 11.272040367126465,
      "learning_rate": 1.7833333333333334e-05,
      "loss": 2.3349,
      "step": 1930
    },
    {
      "epoch": 0.1757724019208118,
      "grad_norm": 10.969583511352539,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 2.1519,
      "step": 1940
    },
    {
      "epoch": 0.17667844522968199,
      "grad_norm": 12.628972053527832,
      "learning_rate": 1.75e-05,
      "loss": 2.2287,
      "step": 1950
    },
    {
      "epoch": 0.17758448853855213,
      "grad_norm": 11.979259490966797,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 2.1451,
      "step": 1960
    },
    {
      "epoch": 0.1784905318474223,
      "grad_norm": 9.67258358001709,
      "learning_rate": 1.7166666666666666e-05,
      "loss": 1.8779,
      "step": 1970
    },
    {
      "epoch": 0.17939657515629248,
      "grad_norm": 21.925521850585938,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 2.1461,
      "step": 1980
    },
    {
      "epoch": 0.18030261846516263,
      "grad_norm": 12.360406875610352,
      "learning_rate": 1.6833333333333334e-05,
      "loss": 2.0484,
      "step": 1990
    },
    {
      "epoch": 0.1812086617740328,
      "grad_norm": 12.042925834655762,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 2.1817,
      "step": 2000
    },
    {
      "epoch": 0.1812086617740328,
      "eval_bleu-4": 0.19190087217826146,
      "eval_rouge-1": 48.829716,
      "eval_rouge-2": 24.751032,
      "eval_rouge-l": 47.297242,
      "eval_runtime": 8.3262,
      "eval_samples_per_second": 6.005,
      "eval_steps_per_second": 0.48,
      "step": 2000
    },
    {
      "epoch": 0.18211470508290295,
      "grad_norm": 11.616395950317383,
      "learning_rate": 1.65e-05,
      "loss": 2.2413,
      "step": 2010
    },
    {
      "epoch": 0.18302074839177312,
      "grad_norm": 7.805538177490234,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 1.6702,
      "step": 2020
    },
    {
      "epoch": 0.1839267917006433,
      "grad_norm": 8.162835121154785,
      "learning_rate": 1.6166666666666665e-05,
      "loss": 2.2791,
      "step": 2030
    },
    {
      "epoch": 0.18483283500951345,
      "grad_norm": 13.158295631408691,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.5357,
      "step": 2040
    },
    {
      "epoch": 0.18573887831838362,
      "grad_norm": 12.625104904174805,
      "learning_rate": 1.5833333333333333e-05,
      "loss": 2.1497,
      "step": 2050
    },
    {
      "epoch": 0.1866449216272538,
      "grad_norm": 12.165435791015625,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 2.1332,
      "step": 2060
    },
    {
      "epoch": 0.18755096493612394,
      "grad_norm": 11.673022270202637,
      "learning_rate": 1.55e-05,
      "loss": 2.2564,
      "step": 2070
    },
    {
      "epoch": 0.18845700824499412,
      "grad_norm": 12.212632179260254,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 2.2688,
      "step": 2080
    },
    {
      "epoch": 0.18936305155386426,
      "grad_norm": 7.020449638366699,
      "learning_rate": 1.5166666666666668e-05,
      "loss": 2.1648,
      "step": 2090
    },
    {
      "epoch": 0.19026909486273444,
      "grad_norm": 9.245616912841797,
      "learning_rate": 1.5e-05,
      "loss": 2.3773,
      "step": 2100
    },
    {
      "epoch": 0.1911751381716046,
      "grad_norm": 16.39710807800293,
      "learning_rate": 1.4833333333333336e-05,
      "loss": 2.3243,
      "step": 2110
    },
    {
      "epoch": 0.19208118148047476,
      "grad_norm": 13.499908447265625,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 2.2926,
      "step": 2120
    },
    {
      "epoch": 0.19298722478934494,
      "grad_norm": 13.690010070800781,
      "learning_rate": 1.45e-05,
      "loss": 2.4147,
      "step": 2130
    },
    {
      "epoch": 0.19389326809821508,
      "grad_norm": 9.954277038574219,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 1.5644,
      "step": 2140
    },
    {
      "epoch": 0.19479931140708526,
      "grad_norm": 9.106829643249512,
      "learning_rate": 1.4166666666666668e-05,
      "loss": 1.9758,
      "step": 2150
    },
    {
      "epoch": 0.19570535471595543,
      "grad_norm": 9.856226921081543,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 2.4281,
      "step": 2160
    },
    {
      "epoch": 0.19661139802482558,
      "grad_norm": 9.533503532409668,
      "learning_rate": 1.3833333333333334e-05,
      "loss": 2.3351,
      "step": 2170
    },
    {
      "epoch": 0.19751744133369575,
      "grad_norm": 6.99009895324707,
      "learning_rate": 1.3666666666666666e-05,
      "loss": 1.8818,
      "step": 2180
    },
    {
      "epoch": 0.19842348464256593,
      "grad_norm": 7.16886043548584,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 2.3228,
      "step": 2190
    },
    {
      "epoch": 0.19932952795143608,
      "grad_norm": 14.84598159790039,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 2.2998,
      "step": 2200
    },
    {
      "epoch": 0.20023557126030625,
      "grad_norm": 12.393861770629883,
      "learning_rate": 1.3166666666666665e-05,
      "loss": 2.0962,
      "step": 2210
    },
    {
      "epoch": 0.2011416145691764,
      "grad_norm": 9.340067863464355,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 1.944,
      "step": 2220
    },
    {
      "epoch": 0.20204765787804657,
      "grad_norm": 11.241460800170898,
      "learning_rate": 1.2833333333333333e-05,
      "loss": 1.7996,
      "step": 2230
    },
    {
      "epoch": 0.20295370118691675,
      "grad_norm": 10.771415710449219,
      "learning_rate": 1.2666666666666668e-05,
      "loss": 2.2998,
      "step": 2240
    },
    {
      "epoch": 0.2038597444957869,
      "grad_norm": 10.155713081359863,
      "learning_rate": 1.25e-05,
      "loss": 1.9233,
      "step": 2250
    },
    {
      "epoch": 0.20476578780465707,
      "grad_norm": 9.167989730834961,
      "learning_rate": 1.2333333333333334e-05,
      "loss": 2.083,
      "step": 2260
    },
    {
      "epoch": 0.20567183111352721,
      "grad_norm": 13.440271377563477,
      "learning_rate": 1.2166666666666668e-05,
      "loss": 2.4269,
      "step": 2270
    },
    {
      "epoch": 0.2065778744223974,
      "grad_norm": 8.592093467712402,
      "learning_rate": 1.2e-05,
      "loss": 2.0824,
      "step": 2280
    },
    {
      "epoch": 0.20748391773126756,
      "grad_norm": 9.669317245483398,
      "learning_rate": 1.1833333333333334e-05,
      "loss": 2.188,
      "step": 2290
    },
    {
      "epoch": 0.2083899610401377,
      "grad_norm": 9.269598960876465,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 2.5854,
      "step": 2300
    },
    {
      "epoch": 0.20929600434900789,
      "grad_norm": 14.334810256958008,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 2.2236,
      "step": 2310
    },
    {
      "epoch": 0.21020204765787803,
      "grad_norm": 12.318116188049316,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 2.3935,
      "step": 2320
    },
    {
      "epoch": 0.2111080909667482,
      "grad_norm": 9.796751976013184,
      "learning_rate": 1.1166666666666668e-05,
      "loss": 2.0815,
      "step": 2330
    },
    {
      "epoch": 0.21201413427561838,
      "grad_norm": 14.383096694946289,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.8493,
      "step": 2340
    },
    {
      "epoch": 0.21292017758448853,
      "grad_norm": 10.11276912689209,
      "learning_rate": 1.0833333333333334e-05,
      "loss": 1.8068,
      "step": 2350
    },
    {
      "epoch": 0.2138262208933587,
      "grad_norm": 12.265495300292969,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 2.1102,
      "step": 2360
    },
    {
      "epoch": 0.21473226420222888,
      "grad_norm": 13.710020065307617,
      "learning_rate": 1.05e-05,
      "loss": 2.0866,
      "step": 2370
    },
    {
      "epoch": 0.21563830751109903,
      "grad_norm": 9.320695877075195,
      "learning_rate": 1.0333333333333333e-05,
      "loss": 2.1293,
      "step": 2380
    },
    {
      "epoch": 0.2165443508199692,
      "grad_norm": 7.855961799621582,
      "learning_rate": 1.0166666666666667e-05,
      "loss": 2.3985,
      "step": 2390
    },
    {
      "epoch": 0.21745039412883935,
      "grad_norm": 11.76258659362793,
      "learning_rate": 1e-05,
      "loss": 2.1339,
      "step": 2400
    },
    {
      "epoch": 0.21835643743770952,
      "grad_norm": 28.065231323242188,
      "learning_rate": 9.833333333333333e-06,
      "loss": 2.2889,
      "step": 2410
    },
    {
      "epoch": 0.2192624807465797,
      "grad_norm": 8.632898330688477,
      "learning_rate": 9.666666666666667e-06,
      "loss": 2.2781,
      "step": 2420
    },
    {
      "epoch": 0.22016852405544984,
      "grad_norm": 11.706769943237305,
      "learning_rate": 9.5e-06,
      "loss": 2.0964,
      "step": 2430
    },
    {
      "epoch": 0.22107456736432002,
      "grad_norm": 15.167389869689941,
      "learning_rate": 9.333333333333334e-06,
      "loss": 2.1776,
      "step": 2440
    },
    {
      "epoch": 0.22198061067319017,
      "grad_norm": 9.513143539428711,
      "learning_rate": 9.166666666666666e-06,
      "loss": 2.2047,
      "step": 2450
    },
    {
      "epoch": 0.22288665398206034,
      "grad_norm": 10.837373733520508,
      "learning_rate": 9e-06,
      "loss": 1.7987,
      "step": 2460
    },
    {
      "epoch": 0.22379269729093051,
      "grad_norm": 12.232101440429688,
      "learning_rate": 8.833333333333334e-06,
      "loss": 2.0873,
      "step": 2470
    },
    {
      "epoch": 0.22469874059980066,
      "grad_norm": 14.896476745605469,
      "learning_rate": 8.666666666666668e-06,
      "loss": 2.3756,
      "step": 2480
    },
    {
      "epoch": 0.22560478390867084,
      "grad_norm": 13.874653816223145,
      "learning_rate": 8.500000000000002e-06,
      "loss": 2.2866,
      "step": 2490
    },
    {
      "epoch": 0.226510827217541,
      "grad_norm": 9.589611053466797,
      "learning_rate": 8.333333333333334e-06,
      "loss": 2.0027,
      "step": 2500
    },
    {
      "epoch": 0.226510827217541,
      "eval_bleu-4": 0.17600807694688408,
      "eval_rouge-1": 45.066214,
      "eval_rouge-2": 22.168244,
      "eval_rouge-l": 44.330562,
      "eval_runtime": 8.9361,
      "eval_samples_per_second": 5.595,
      "eval_steps_per_second": 0.448,
      "step": 2500
    }
  ],
  "logging_steps": 10,
  "max_steps": 3000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "total_flos": 1.34388395630592e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
